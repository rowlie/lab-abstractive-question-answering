{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7KBOS0yO_mi"
      },
      "source": [
        "# LAB | Abstractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL4goh8fPC5l"
      },
      "source": [
        "Abstractive question-answering focuses on the generation of multi-sentence answers to open-ended questions. It usually works by searching massive document stores for relevant information and then using this information to synthetically generate answers. This notebook demonstrates how Pinecone helps you build an abstractive question-answering system. We need three main components:\n",
        "\n",
        "- A vector index to store and run semantic search\n",
        "- A retriever model for embedding context passages\n",
        "- A generator model to generate answers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load a small, lightweight model\n",
        "retriever = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "\n",
        "print(\"Retriever loaded successfully on\", device)"
      ],
      "metadata": {
        "id": "533jNEqej4If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6uNNJmuPIVT"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZINfOdPyJHBZ"
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets==2.16.1 pinecone-client==3.1.0 sentence-transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve the API keys\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# Set as environment variables\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "\n",
        "print(\"OpenAI API key loaded and set as environment variable.\")\n",
        "print(\"Pinecone API key loaded and set as environment variable.\")"
      ],
      "metadata": {
        "id": "FS3q0TvWd7xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0EU4eIbPTTL"
      },
      "source": [
        "# Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USvwSnsDvrdz"
      },
      "source": [
        "Our source data will be taken from the Wiki Snippets dataset, which contains over 17 million passages from Wikipedia. But, since indexing the entire dataset may take some time, we will only utilize 50,000 passages in this demo that include \"History\" in the \"section title\" column. If you want, you may utilize the complete dataset. Pinecone vector database can effortlessly manage millions of documents for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bShG-f5IPPtG"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# load the SQuAD dataset which contains Wikipedia contexts\n",
        "# We will use streaming mode and shuffle it\n",
        "wiki_data = load_dataset(\n",
        "  'wiki_snippets',\n",
        "  'wikipedia_en_100_0', # Use a valid BuilderConfig\n",
        "  split='train',\n",
        "  streaming=True\n",
        ").shuffle(seed=960)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoPdaLFAvvci"
      },
      "source": [
        "We are loading the dataset in the streaming mode so that we don't have to wait for the whole dataset to download (which is over 9GB). Instead, we iteratively download records one at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLjXCwKevy4K"
      },
      "outputs": [],
      "source": [
        "# show the contents of a single document in the dataset\n",
        "next(iter(wiki_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHYNM6VXv1MJ"
      },
      "outputs": [],
      "source": [
        "# filter only documents with History as section_title\n",
        "history = wiki_data.filter(lambda x: 'History' in x.get('section_title', ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8sTBqS0v4RM"
      },
      "source": [
        "Let's iterate through the dataset and apply our filter to select the 50,000 historical passages. We will extract `article_title`, `section_title` and `passage_text` from each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob0prowIzjJ9"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm  # progress bar\n",
        "\n",
        "total_doc_count = 50000\n",
        "\n",
        "counter = 0\n",
        "docs = []\n",
        "# iterate through the dataset and apply our filter\n",
        "for d in tqdm(history, total=total_doc_count):\n",
        "    # extract the fields we need - article, section, and passage\n",
        "    article_title = d.get('article_title', '')\n",
        "    section_title = d.get('section_title', '')\n",
        "    passage_text = d.get('passage_text', '') # Corrected from 'context' to 'passage_text'\n",
        "    if passage_text: # Only append if passage_text is not empty\n",
        "        docs.append({\"article_title\": article_title, \"section_title\": section_title, \"passage_text\": passage_text})\n",
        "        # increase the counter on every iteration\n",
        "        counter += 1\n",
        "        if counter == total_doc_count:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvTA40mq5FKj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a pandas dataframe with the documents we extracted\n",
        "df = pd.DataFrame(docs)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDYP3RkcPYdf"
      },
      "source": [
        "# Initialize Pinecone Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5m5HHVu4War"
      },
      "source": [
        "The Pinecone index stores vector representations of our historical passages which we can retrieve later using another vector (query vector). To build our vector index, we must first establish a connection with Pinecone. For this, we need an API from Pinecone. You can get one for free from [here](https://app.pinecone.io/), and after that, we initialize the connection as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS5csvn4dHGB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1mwuxgPdHGC"
      },
      "source": [
        "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26CLHOMwdHGC"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CPJmTIy4XsL"
      },
      "source": [
        "Now we create a new index. We will name it \"abstractive-question-answering\" — you can name it anything we want. We specify the metric type as \"cosine\" and dimension as 768 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 768-dimension vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RlNTDAcdHGC"
      },
      "outputs": [],
      "source": [
        "index_name = 'abstractive-qa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSiAgiOwdHGC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes():\n",
        "    # create a new index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=768,  # dimensionality of SentenceTransformer embeddings\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status.ready:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to the index\n",
        "index = pc.Index(index_name)\n",
        "# view index statistics\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHcCQVi4PcmE"
      },
      "source": [
        "# Initialize Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udmF-qY24yQ1"
      },
      "source": [
        "Next, we need to initialize our retriever. The retriever will mainly do two things:\n",
        "\n",
        "- Generate embeddings for all historical passages (context vectors/embeddings)\n",
        "- Generate embeddings for our questions (query vector/embedding)\n",
        "\n",
        "The retriever will create embeddings such that the questions and passages that hold the answers to our queries are close to one another in the vector space. We will use a SentenceTransformer model based on Microsoft's MPNet as our retriever. This model performs quite well for comparing the similarity between queries and documents. We can use Cosine Similarity to compute the similarity between query and context vectors generated by this model (Pinecone automatically does this for us)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45dRrEZNPdXu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# set device to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# load the retriever model from huggingface model hub\n",
        "retriever = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base', device=device)\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo5MN_GKPeDy"
      },
      "source": [
        "# Generate Embeddings and Upsert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj0NdB-kU9Yt"
      },
      "source": [
        "Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, section title, passage text, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OqB7bBePia-"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm  # progress bar\n",
        "\n",
        "# we will use batches of 64\n",
        "batch_size = 64\n",
        "\n",
        "#You will create embedding for the passage_text variable and be use to include the meta data in each batch\n",
        "for i in tqdm(range(0, len(df), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(i + batch_size, len(df))\n",
        "    # extract batch\n",
        "    batch = df.iloc[i:i_end]\n",
        "    # generate embeddings for batch\n",
        "    emb = retriever.encode(batch['passage_text'].tolist()).tolist()\n",
        "    # create Pinecone records\n",
        "    records = []\n",
        "    for idx, row in batch.iterrows():\n",
        "        records.append({\n",
        "            'id': str(idx),\n",
        "            'values': emb[idx - i],\n",
        "            'metadata': {\n",
        "                'article_title': row['article_title'],\n",
        "                'section_title': row['section_title'],\n",
        "                'passage_text': row['passage_text']\n",
        "            }\n",
        "        })\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=records)\n",
        "# check that we have all vectors in index\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFKPJ0WtPmAw"
      },
      "source": [
        "# Initialize Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RraVCG_hcv3"
      },
      "source": [
        "We will use ELI5 BART for the generator which is a Sequence-To-Sequence model trained using the ‘Explain Like I’m 5’ (ELI5) dataset. Sequence-To-Sequence models can take a text sequence as input and produce a different text sequence as output.\n",
        "\n",
        "The input to the ELI5 BART model is a single string which is a concatenation of the query and the relevant documents providing the context for the answer. The documents are separated by a special token &lt;P>, so the input string will look as follows:\n",
        "\n",
        ">question: What is a sonic boom? context: &lt;P> A sonic boom is a sound associated with shock waves created when an object travels through the air faster than the speed of sound. &lt;P> Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. &lt;P> Sonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. This led to prohibition of routine supersonic flight overland.\n",
        "\n",
        "More detail on how the ELI5 dataset was built is available [here](https://arxiv.org/abs/1907.09190) and how ELI5 BART model was trained is available [here](https://yjernite.github.io/lfqa.html).\n",
        "\n",
        "Let's initialize the BART model using transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76QyADZ0Pqki"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# load bart tokenizer and model from huggingface\n",
        "tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\n",
        "generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBr1GqvwkI3W"
      },
      "source": [
        "All the components of our abstract QA system are complete and ready to be queried. But first, let's write some helper functions to retrieve context passages from Pinecone index and to format the query in the way the generator expects the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJfIe7CtlsWo"
      },
      "outputs": [],
      "source": [
        "def query_pinecone(query, top_k):\n",
        "    # generate embeddings for the query\n",
        "    xq = retriever.encode(query).tolist()\n",
        "    # search pinecone index for context passage with the answer\n",
        "    xc = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
        "    return xc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiOZ2yWDHI8p"
      },
      "outputs": [],
      "source": [
        "def format_query(query, context):\n",
        "    # extract passage_text from Pinecone search result and add the <P> tag\n",
        "    context = [f\"<P> {m['metadata']['passage_text']}\" for m in context]\n",
        "    # concatinate all context passages\n",
        "    context = ''.join(context)\n",
        "    # contcatinate the query and context passages\n",
        "    query = f\"question: {query} context: {context}\"\n",
        "    return query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA0f1Xyxq-Fp"
      },
      "source": [
        "Let's test the helper functions. We will query the Pinecone index function we created earlier with the `query_pinecone` to get context passages and pass them to the `format_query` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSm2nj11mR87"
      },
      "outputs": [],
      "source": [
        "query = \"when was the first electric power system built?\"\n",
        "result = query_pinecone(query, top_k=1)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yoylQFO4joX"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPxVoo6am8ls"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# format the query in the form generator expects the input\n",
        "query = format_query(query, result[\"matches\"])\n",
        "pprint(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY8LSqk7rdPS"
      },
      "source": [
        "The output looks great. Now let's write a function to generate answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fxIoNqeoEPD"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query):\n",
        "    # tokenize the query to get input_ids\n",
        "    inputs = tokenizer([query], max_length=1024, return_tensors=\"pt\").to(device)\n",
        "    # use generator to predict output ids\n",
        "    ids = generator.generate(inputs[\"input_ids\"], num_beams=2, min_length=20, max_length=40)\n",
        "    # use tokenizer to decode the output ids\n",
        "    answer = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    return pprint(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYXFSBqSsZMx"
      },
      "outputs": [],
      "source": [
        "generate_answer(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYWACzIotWQY"
      },
      "source": [
        "As we can see, the generator used the provided context to answer our question. Let's run some more queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iFLxPPvx2Tm"
      },
      "outputs": [],
      "source": [
        "query = \"How was the first wireless message sent?\"\n",
        "context = query_pinecone(query, top_k=5)\n",
        "query = format_query(query, context[\"matches\"])\n",
        "generate_answer(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI2fTt_A5Ozf"
      },
      "source": [
        "To confirm that this answer is correct, we can check the contexts used to generate the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15WZSeZw3mNY"
      },
      "outputs": [],
      "source": [
        "for doc in context[\"matches\"]:\n",
        "    print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH-K7WHR9z8m"
      },
      "source": [
        "In this case, the answer looks correct. If we ask a question and no relevant contexts are retrieved, the generator will typically return nonsensical or false answers, like with this question about COVID-19:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1naD6pRlmWd"
      },
      "outputs": [],
      "source": [
        "query = \"where did COVID-19 originate?\"\n",
        "context = query_pinecone(query, top_k=3)\n",
        "query = format_query(query, context[\"matches\"])\n",
        "generate_answer(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3gX5xQY-jKJ"
      },
      "outputs": [],
      "source": [
        "for doc in context[\"matches\"]:\n",
        "    print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCUqB_MrAABI"
      },
      "source": [
        "Let’s finish with a final few questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_oWypShW_rk"
      },
      "outputs": [],
      "source": [
        "query = \"what was the war of currents?\"\n",
        "context = query_pinecone(query, top_k=5)\n",
        "query = format_query(query, context[\"matches\"])\n",
        "generate_answer(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ1-OsmT0duI"
      },
      "outputs": [],
      "source": [
        "query = \"who was the first person on the moon?\"\n",
        "context = query_pinecone(query, top_k=10)\n",
        "query = format_query(query, context[\"matches\"])\n",
        "generate_answer(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU6DH-ah1g1t"
      },
      "outputs": [],
      "source": [
        "query = \"what was NASAs most expensive project?\"\n",
        "context = query_pinecone(query, top_k=3)\n",
        "query = format_query(query, context[\"matches\"])\n",
        "generate_answer(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-cQPSkG9cFY"
      },
      "source": [
        "As we can see, the model can generate some decent answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi5-s5tJdHGG"
      },
      "source": [
        "#### Add a few more questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUBmzEtndHGG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}